{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "config.json",
  "$ref": "#/$defs/SerializedContinueConfig",
  "$defs": {
    "BaseCompletionOptions": {
      "title": "BaseCompletionOptions",
      "type": "object",
      "properties": {
        "stream": {
          "title": "Stream",
          "description": "Whether to stream the LLM response. Currently only respected by the 'anthropic' provider. Otherwise will always stream.",
          "type": "boolean",
          "default": true
        },
        "temperature": {
          "title": "Temperature",
          "description": "The temperature of the completion.",
          "type": "number"
        },
        "topP": {
          "title": "Top P",
          "description": "The topP of the completion.",
          "type": "number"
        },
        "topK": {
          "title": "Top K",
          "description": "The topK of the completion.",
          "type": "integer"
        },
        "presencePenalty": {
          "title": "Presence Penalty",
          "description": "The presence penalty of the completion.",
          "type": "number"
        },
        "frequencePenalty": {
          "title": "Frequency Penalty",
          "description": "The frequency penalty of the completion.",
          "type": "number"
        },
        "mirostat": {
          "title": "Mirostat",
          "description": "Enable Mirostat sampling, controlling perplexity during text generation (default: 0). Only available for Ollama, LM Studio, and llama.cpp providers.",
          "type": "number"
        },
        "mirostatEta": {
          "title": "Mirostat Eta",
          "description": "Controls how quickly the model converges to its optimal perplexity. Only enabled when `mirostat` is also enabled.",
          "type": "number"
        },
        "mirostatTau": {
          "title": "Mirostat Tau",
          "description": "Controls how quickly the model converges to its optimal perplexity. Only enabled when `mirostat` is also enabled.",
          "type": "number"
        },
        "stop": {
          "title": "Stop",
          "description": "The stop tokens of the completion.",
          "type": "array",
          "items": {
            "type": "string"
          }
        },
        "maxTokens": {
          "title": "Max Tokens",
          "description": "The maximum number of tokens to generate.",
          "default": 600,
          "type": "integer"
        },
        "numThreads": {
          "title": "Number of threads",
          "description": "The number of threads used in the generation process. Only available for Ollama (this is the num_thread parameter)",
          "type": "integer"
        },
        "useMmap": {
          "title": "Use Memory Map",
          "description": "Allows the model to be mapped into memory. If disabled, can enhance response time on low-end devices. Only available for Ollama.",
          "type": "boolean"
        },
        "keepAlive": {
          "title": "Ollama keep_alive",
          "description": "The number of seconds after no requests are made to unload the model from memory. Defaults to 1800 (30min).",
          "type": "integer"
        }
      }
    },
    "ClientCertificateOptions": {
      "title": "ClientCertificateOptions",
      "type": "object",
      "properties": {
        "cert": {
          "title": "Cert Path",
          "description": "Path to the client certificate file",
          "type": "string"
        },
        "key": {
          "title": "Key Path",
          "description": "Path to the client certificate key file",
          "type": "string"
        },
        "passphrase": {
          "title": "Passphrase",
          "description": "Passphrase for the client certificate key file",
          "type": "string"
        }
      },
      "required": ["cert", "key"]
    },
    "RequestOptions": {
      "title": "RequestOptions",
      "type": "object",
      "properties": {
        "timeout": {
          "title": "Timeout",
          "description": "Set the timeout for each request to the LLM. If you are running a local LLM that takes a while to respond, you might want to increase this.",
          "default": 7200,
          "type": "integer"
        },
        "verifySsl": {
          "title": "Verify SSL",
          "description": "Whether to verify SSL certificates for requests.",
          "type": "boolean"
        },
        "caBundlePath": {
          "title": "Ca Bundle Path",
          "description": "Path to a custom CA bundle to use when making the HTTP request",
          "anyOf": [
            { "type": "string" },
            {
              "type": "array",
              "items": { "type": "string" }
            }
          ]
        },
        "proxy": {
          "title": "Proxy",
          "description": "Proxy URL to use when making the HTTP request",
          "type": "string"
        },
        "headers": {
          "title": "Headers",
          "description": "Headers to use when making the HTTP request",
          "type": "object",
          "additionalProperties": { "type": "string" }
        },
        "extraBodyProperties": {
          "title": "Extra Body Properties",
          "description": "This object will be merged with the body when making the HTTP requests",
          "type": "object"
        },
        "noProxy": {
          "title": "No Proxy",
          "description": "A list of hostnames for which Continue should not use the proxy specified in requestOptions.proxy",
          "type": "array",
          "items": { "type": "string" }
        },
        "clientCertificate": {
          "title": "Client Certificate",
          "description": "Client certificate to use when making the HTTP request",
          "$ref": "#/$defs/ClientCertificateOptions"
        }
      }
    },
    "ModelDescription": {
      "title": "ModelDescription",
      "type": "object",
      "properties": {
        "title": {
          "title": "Title",
          "description": "The title you wish to give your model.",
          "type": "string"
        },
        "provider": {
          "title": "Provider",
          "description": "The provider of the model. This is used to determine how to interact with it.",
          "enum": [
            "openai",
            "free-trial",
            "anthropic",
            "cohere",
            "bedrock",
            "bedrockimport",
            "sagemaker",
            "together",
            "ollama",
            "huggingface-tgi",
            "huggingface-inference-api",
            "llama.cpp",
            "replicate",
            "gemini",
            "lmstudio",
            "llamafile",
            "mistral",
            "deepinfra",
            "groq",
            "fireworks",
            "cloudflare",
            "deepseek",
            "azure",
            "msty",
            "watsonx",
            "openrouter",
            "sambanova",
            "nvidia",
            "vllm",
            "cerebras",
            "askSage",
            "nebius",
            "vertexai",
            "xAI",
            "kindo",
            "moonshot",
            "siliconflow",
            "function-network",
            "scaleway",
            "mlx_lm"
          ],
          "markdownEnumDescriptions": [
            "### OpenAI ...",
            "### Free Trial ...",
            "### Anthropic ...",
            "### Cohere ...",
            "### Bedrock ...",
            "### Bedrock Imported Models ...",
            "### Sagemaker ...",
            "### Together ...",
            "### Ollama ...",
            "### Huggingface TGI ...",
            "### Huggingface Inference API ...",
            "### Llama.cpp ...",
            "### Replicate ...",
            "### Gemini API ...",
            "### Gemini API on Vertex AI ...",
            "### LMStudio ...",
            "### Llamafile ...",
            "### Mistral API ...",
            "### Mistral API on Vertex AI ...",
            "### DeepInfra ...",
            "### Groq ...",
            "### Fireworks ...",
            "### Cloudflare Workers AI ...",
            "### Deepseek ...",
            "### Azure OpenAI ...",
            "### Msty ...",
            "### IBM watsonx ...",
            "### OpenRouter ...",
            "### NVIDIA NIMs ...",
            "### vLLM ...",
            "### Cerebras ...",
            "### Ask Sage ...",
            "### xAI ...",
            "### Kindo ...",
            "### Moonshot ...",
            "### SiliconFlow ...",
            "### Function Network ...",
            "### Scaleway ...",
            "### MlxLm\nYou can connect to an MLX server running at http://localhost:8000/ or wherever your server is hosted. Provide optional `apiKey` if needed for authorization."
          ],
          "type": "string"
        },
        "model": {
          "title": "Model",
          "description": "The name of the model. Used to autodetect prompt template or to store your server's model ID.",
          "type": "string"
        },
        "apiKey": {
          "title": "Api Key",
          "description": "API key if the provider requires one (e.g. OpenAI, Anthropic, etc.)",
          "type": "string"
        },
        "apiBase": {
          "title": "Api Base",
          "description": "The base URL of the LLM API.",
          "type": "string"
        },
        "region": {
          "title": "Region",
          "description": "The region where the model is hosted",
          "anyOf": [
            {
              "enum": [
                "us-east-1",
                "us-east-2",
                "us-west-1",
                "us-west-2",
                "eu-west-1",
                "eu-central-1",
                "ap-southeast-1",
                "ap-northeast-1",
                "ap-south-1",
                "us-central1",
                "us-east1",
                "us-east4",
                "us-east5",
                "us-west1",
                "us-west4",
                "us-south1",
                "northamerica-northeast1",
                "southamerica-east1",
                "europe-central2",
                "europe-north1",
                "europe-west1",
                "europe-west2",
                "europe-west3",
                "europe-west4",
                "europe-west6",
                "europe-west8",
                "europe-west9",
                "europe-southwest1",
                "asia-east1",
                "asia-east2",
                "asia-south1",
                "asia-northeast1",
                "asia-northeast3",
                "asia-southeast1",
                "australia-southeast1",
                "me-central1",
                "me-central2",
                "me-west1"
              ],
              "type": "string"
            },
            { "type": "string" }
          ]
        },
        "profile": {
          "title": "Profile",
          "description": "The AWS security profile to use",
          "type": "string"
        },
        "modelArn": {
          "title": "Model ARN",
          "description": "The AWS arn for the imported model",
          "type": "string"
        },
        "contextLength": {
          "title": "Context Length",
          "description": "The maximum context length of the LLM in tokens, as counted by countTokens.",
          "default": 2048,
          "type": "integer"
        },
        "maxStopWords": {
          "title": "Max Stop Words",
          "description": "The maximum number of stop words that the API will accept. Rarely needed, unless your provider has a strict limit.",
          "type": "integer"
        },
        "template": {
          "title": "Template",
          "description": "The chat template used to format messages. This is auto-detected for most models, but can be overridden here. Choose 'none' if your server handles prompting itself.",
          "enum": [
            "llama2",
            "alpaca",
            "zephyr",
            "phi2",
            "phind",
            "anthropic",
            "chatml",
            "none",
            "deepseek",
            "openchat",
            "xwin-coder",
            "neural-chat",
            "codellama-70b",
            "llava",
            "gemma",
            "llama3"
          ],
          "type": "string"
        },
        "promptTemplates": {
          "title": "Prompt Templates",
          "markdownDescription": "A mapping of template name to the string used. See docs for how to customize the 'edit' prompt or other prompts.",
          "type": "object",
          "additionalProperties": {
            "type": "string"
          }
        },
        "completionOptions": {
          "title": "Completion Options",
          "description": "Options for the completion endpoint. Read more in the docs.",
          "default": {
            "temperature": null,
            "topP": null,
            "topK": null,
            "presencePenalty": null,
            "frequencyPenalty": null,
            "stop": null,
            "maxTokens": 2048
          },
          "allOf": [{ "$ref": "#/$defs/BaseCompletionOptions" }]
        },
        "systemMessage": {
          "title": "System Message",
          "description": "A system message that will always be prepended when sending requests to the LLM",
          "type": "string"
        },
        "requestOptions": {
          "title": "Request Options",
          "description": "Options for the HTTP request to the LLM.",
          "default": {
            "timeout": 7200,
            "verifySsl": null,
            "caBundlePath": null,
            "proxy": null,
            "headers": null,
            "extraBodyProperties": null
          },
          "allOf": [{ "$ref": "#/$defs/RequestOptions" }]
        },
        "apiType": {
          "title": "Api Type",
          "markdownDescription": "OpenAI API type, either `openai` or `azure`",
          "enum": ["openai", "azure"]
        },
        "apiVersion": {
          "title": "Api Version",
          "description": "Azure OpenAI API version (e.g. 2023-07-01-preview)",
          "type": "string"
        },
        "deployment": {
          "title": "Deployment",
          "description": "Azure OpenAI deployment",
          "type": "string"
        },
        "capabilities": {
          "type": "object",
          "description": "Override model capabilities if auto-detection is incorrect.",
          "properties": {
            "uploadImage": {
              "type": "boolean",
              "description": "Indicates whether the model can upload images."
            }
          }
        }
      },
      "required": ["title", "provider", "model"],
      "allOf": [
        {
          "if": {
            "properties": {
              "provider": {
                "type": "str"
              }
            },
            "not": { "required": ["provider"] }
          },
          "then": {
            "properties": {
              "model": {
                "description": "Choose a provider first, then model options will be shown here."
              }
            }
          }
        },
        {
          "if": {
            "properties": {
              "provider": {
                "enum": [
                  "openai",
                  "anthropic",
                  "cohere",
                  "gemini",
                  "huggingface-inference-api",
                  "replicate",
                  "together",
                  "cloudflare",
                  "sambanova",
                  "nebius",
                  "xAI",
                  "kindo",
                  "scaleway"
                ]
              }
            },
            "required": ["provider"]
          },
          "then": {
            "required": ["apiKey"]
          }
        },
        {
          "if": {
            "properties": {
              "provider": { "enum": ["bedrockimport"] }
            },
            "required": ["provider"]
          },
          "then": {
            "required": ["modelArn"]
          }
        },
        {
          "if": {
            "properties": {
              "provider": { "enum": ["huggingface-tgi", "huggingface-inference-api"] }
            }
          },
          "then": {
            "required": ["apiBase"]
          },
          "required": ["provider"]
        },
        {
          "if": {
            "properties": {
              "provider": { "enum": ["openai"] }
            },
            "required": ["provider"]
          },
          "then": {
            "properties": {
              "deployment": { "type": "string" },
              "apiType": { "type": "string" },
              "apiVersion": { "type": "string" }
            }
          }
        },
        {
          "if": {
            "properties": {
              "provider": { "enum": ["cloudflare"] }
            },
            "required": ["provider"]
          },
          "then": {
            "properties": {
              "accountId": { "type": "string" },
              "aiGatewaySlug": { "type": "string" },
              "model": {
                "anyOf": [
                  {
                    "enum": [
                      "@cf/meta/llama-3-8b-instruct",
                      "@hf/thebloke/deepseek-coder-6.7b-instruct-awq",
                      "@cf/deepseek-ai/deepseek-math-7b-instruct",
                      "@cf/thebloke/discolm-german-7b-v1-awq",
                      "@cf/tiiuae/falcon-7b-instruct",
                      "@cf/google/gemma-2b-it-lora",
                      "@hf/google/gemma-7b-it",
                      "@cf/google/gemma-7b-it-lora",
                      "@hf/nousresearch/hermes-2-pro-mistral-7b",
                      "@cf/meta/llama-2-7b-chat-fp16",
                      "@cf/meta/llama-2-7b-chat-int8",
                      "@cf/meta-llama/llama-2-7b-chat-hf-lora",
                      "@hf/thebloke/llama-2-13b-chat-awq",
                      "@hf/thebloke/llamaguard-7b-awq",
                      "@cf/mistral/mistral-7b-instruct-v0.1",
                      "@hf/mistral/mistral-7b-instruct-v0.2",
                      "@cf/mistral/mistral-7b-instruct-v0.2-lora",
                      "@hf/thebloke/neural-chat-7b-v3-1-awq",
                      "@cf/openchat/openchat-3.5-0106",
                      "@hf/thebloke/openhermes-2.5-mistral-7b-awq",
                      "@cf/microsoft/phi-2",
                      "@cf/qwen/qwen1.5-0.5b-chat",
                      "@cf/qwen/qwen1.5-1.8b-chat",
                      "@cf/qwen/qwen1.5-7b-chat-awq",
                      "@cf/qwen/qwen1.5-14b-chat-awq",
                      "@cf/defog/sqlcoder-7b-2",
                      "@hf/nexusflow/starling-lm-7b-beta",
                      "@cf/tinyllama/tinyllama-1.1b-chat-v1.0",
                      "@hf/thebloke/zephyr-7b-beta-awq",
                      "@hf/thebloke/deepseek-coder-6.7b-base-awq"
                    ]
                  },
                  { "type": "string" }
                ]
              }
            }
          }
        },
        {
          "if": {
            "properties": {
              "provider": { "enum": ["openai"] },
              "apiType": { "not": { "const": "azure" } }
            },
            "required": ["provider"]
          },
          "then": {
            "properties": {
              "model": {
                "anyOf": [
                  {
                    "enum": [
                      "gpt-3.5-turbo",
                      "gpt-3.5-turbo-16k",
                      "gpt-4o",
                      "gpt-4o-mini",
                      "gpt-4",
                      "gpt-3.5-turbo-0613",
                      "gpt-4-32k",
                      "gpt-4-turbo",
                      "gpt-4-vision-preview",
                      "mistral-7b",
                      "mistral-8x7b",
                      "llama2-7b",
                      "llama2-13b",
                      "codellama-7b",
                      "codellama-13b",
                      "codellama-34b",
                      "codellama-70b",
                      "llama3-8b",
                      "llama3-70b",
                      "llama3.1-8b",
                      "llama3.1-70b",
                      "llama3.1-405b",
                      "phind-codellama-34b",
                      "wizardcoder-7b",
                      "wizardcoder-13b",
                      "wizardcoder-34b",
                      "zephyr-7b",
                      "codeup-13b",
                      "deepseek-7b",
                      "deepseek-33b",
                      "neural-chat-7b",
                      "deepseek-1b",
                      "stable-code-3b",
                      "starcoder-1b",
                      "starcoder-3b",
                      "starcoder2-3b",
                      "mistral-tiny",
                      "mistral-small",
                      "mistral-medium",
                      "qwen2.5-coder:1.5b",
                      "qwen2.5-coder:3b",
                      "qwen2.5-coder:7b",
                      "qwen2.5-coder:14b",
                      "qwen2.5-coder:32b",
                      "AUTODETECT"
                    ]
                  },
                  { "type": "string" }
                ]
              }
            }
          }
        },
        {
          "if": {
            "properties": { "provider": { "enum": ["anthropic"] } },
            "required": ["provider"]
          },
          "then": {
            "properties": {
              "cacheBehavior": {
                "title": "Caching Behavior",
                "description": "Options for the prompt caching",
                "properties": {
                  "cacheSystemMessage": { "type": "boolean" },
                  "cacheConversation": { "type": "boolean" }
                }
              },
              "model": {
                "anyOf": [
                  {
                    "enum": [
                      "claude-2",
                      "claude-instant-1",
                      "claude-3-5-sonnet-latest",
                      "claude-3-opus-20240229",
                      "claude-3-sonnet-20240229",
                      "claude-3-haiku-20240307",
                      "claude-2.1"
                    ]
                  },
                  { "type": "string" }
                ]
              }
            }
          }
        },
        {
          "if": {
            "properties": { "provider": { "enum": ["cohere"] } },
            "required": ["provider"]
          },
          "then": {
            "properties": {
              "model": {
                "enum": ["command-r", "command-r-plus"]
              }
            }
          }
        },
        {
          "if": {
            "properties": { "provider": { "enum": ["bedrock"] } },
            "required": ["provider"]
          },
          "then": {
            "properties": {
              "model": {
                "anyOf": [
                  {
                    "enum": [
                      "claude-3-sonnet-20240229",
                      "claude-3-haiku-20240307",
                      "claude-2"
                    ]
                  },
                  { "type": "string" }
                ]
              }
            }
          }
        },
        {
          "if": {
            "properties": { "provider": { "enum": ["moonshot"] } },
            "required": ["provider"]
          },
          "then": {
            "properties": {
              "model": { "type": "string" }
            }
          }
        },
        {
          "if": {
            "properties": { "provider": { "enum": ["azure"] } },
            "required": ["provider"]
          },
          "then": {
            "required": ["apiBase", "apiVersion", "deployment"]
          }
        },
        {
          "if": {
            "properties": { "provider": { "enum": ["mlx_lm"] } },
            "required": ["provider"]
          },
          "then": {
            "properties": {
              "apiBase": {
                "title": "Api Base",
                "description": "The base URL of your MlxLm server",
                "type": "string",
                "default": "http://localhost:8000/"
              },
              "apiKey": {
                "title": "Api Key",
                "description": "Bearer token if your MLX server requires authorization",
                "type": "string"
              },
              "model": {
                "title": "Model",
                "description": "Optional model name to send in the request (overridden if server returns a model_id)",
                "type": "string"
              },
              "completionOptions": {
                "title": "Completion Options (MLX)",
                "type": "object",
                "properties": {
                  "stream": {
                    "type": "boolean",
                    "default": true,
                    "description": "MLX server streams responses by default."
                  },
                  "temperature": {
                    "type": "number",
                    "description": "Adjust generation randomness (default 0.7)."
                  },
                  "topP": {
                    "type": "number",
                    "description": "Top-p for nucleus sampling."
                  },
                  "topK": {
                    "type": "integer",
                    "description": "Top-k for sampling."
                  },
                  "stop": {
                    "type": "array",
                    "description": "List of stop strings. The generation will stop if encountered.",
                    "items": { "type": "string" }
                  },
                  "maxTokens": {
                    "type": "integer",
                    "description": "Max tokens to generate (default 600).",
                    "default": 600
                  }
                },
                "additionalProperties": false
              }
            },
            "additionalProperties": false
          }
        }
      ]
    },
    "SerializedContinueConfig": {
      "title": "SerializedContinueConfig",
      "type": "object",
      "properties": {
        "version": {
          "type": "string",
          "description": "Version of the config file"
        },
        "models": {
          "type": "array",
          "items": { "$ref": "#/$defs/ModelDescription" }
        }
      }
    }
  }
}
